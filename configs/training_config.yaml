model_name: google/gemma-3-1b-it
max_seq_length: 1024
lora_rank: 32
max_steps: 250
learning_rate: 5.0e-6
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
num_generations: 4
warmup_ratio: 0.1
weight_decay: 0.1
max_grad_norm: 0.1
log_every: 1
save_every: 250
fast_inference: true
load_in_4bit: true
gpu_memory_utilization: 0.6
report_to: wandb
wandb_project: GRPO-unsloth
